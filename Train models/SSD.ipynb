{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8422500,"sourceType":"datasetVersion","datasetId":5014441},{"sourceId":8488170,"sourceType":"datasetVersion","datasetId":5063745},{"sourceId":8488522,"sourceType":"datasetVersion","datasetId":5064017},{"sourceId":8495398,"sourceType":"datasetVersion","datasetId":5069079}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/tensorflow/models.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-29T11:43:55.989062Z","iopub.execute_input":"2024-05-29T11:43:55.989600Z","iopub.status.idle":"2024-05-29T11:44:15.628795Z","shell.execute_reply.started":"2024-05-29T11:43:55.989572Z","shell.execute_reply":"2024-05-29T11:44:15.627678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Move data set from input path to kaggle/working because in kaggle can't write a file when it is in input path ","metadata":{}},{"cell_type":"code","source":"!mkdir Tensorflow\n!cp -r ./models ./Tensorflow/\n!cp -r /kaggle/input/pokemondetection-ssd/PokemonDetection_SSD/PokemonDetection_SSD ./Tensorflow/","metadata":{"execution":{"iopub.status.busy":"2024-05-29T11:44:15.630305Z","iopub.execute_input":"2024-05-29T11:44:15.630744Z","iopub.status.idle":"2024-05-29T11:45:26.445237Z","shell.execute_reply.started":"2024-05-29T11:44:15.630695Z","shell.execute_reply":"2024-05-29T11:45:26.443830Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%bash\ncd ./Tensorflow/models/research/\nprotoc object_detection/protos/*.proto --python_out=.\ncp object_detection/packages/tf2/setup.py .\npython -m pip install .","metadata":{"execution":{"iopub.status.busy":"2024-05-29T11:45:26.448818Z","iopub.execute_input":"2024-05-29T11:45:26.449684Z","iopub.status.idle":"2024-05-29T11:47:13.680651Z","shell.execute_reply.started":"2024-05-29T11:45:26.449650Z","shell.execute_reply":"2024-05-29T11:47:13.679809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%bash\n# cd ./Tensorflow/models/research/\n# protoc object_detection/protos/*.proto --python_out=.\n# cp object_detection/packages/tf1/setup.py .\n# python -m pip install .","metadata":{"execution":{"iopub.status.busy":"2024-05-29T11:47:13.681789Z","iopub.execute_input":"2024-05-29T11:47:13.682048Z","iopub.status.idle":"2024-05-29T11:47:13.686012Z","shell.execute_reply.started":"2024-05-29T11:47:13.682025Z","shell.execute_reply":"2024-05-29T11:47:13.685151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Must install tensorflow == 2.15 because if install tensorflow <2.13 -> can not use gpu -> use cpu is too long (check tensorflow and CUDA cuDNN board for more ìnformation, if install tensorflow == 2.16 and error will occur while training api     ","metadata":{}},{"cell_type":"code","source":"!pip install tensorflow==2.15.1","metadata":{"execution":{"iopub.status.busy":"2024-05-29T11:47:13.687267Z","iopub.execute_input":"2024-05-29T11:47:13.687550Z","iopub.status.idle":"2024-05-29T11:47:55.789463Z","shell.execute_reply.started":"2024-05-29T11:47:13.687526Z","shell.execute_reply":"2024-05-29T11:47:55.788313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install tensorflow==1.15","metadata":{"execution":{"iopub.status.busy":"2024-05-29T11:47:55.790948Z","iopub.execute_input":"2024-05-29T11:47:55.791283Z","iopub.status.idle":"2024-05-29T11:47:55.795978Z","shell.execute_reply.started":"2024-05-29T11:47:55.791253Z","shell.execute_reply":"2024-05-29T11:47:55.794994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"check if it can detect gpu  ","metadata":{}},{"cell_type":"code","source":"from tensorflow.python.client import device_lib\ndevice_lib.list_local_devices()","metadata":{"execution":{"iopub.status.busy":"2024-05-29T11:47:55.797191Z","iopub.execute_input":"2024-05-29T11:47:55.797540Z","iopub.status.idle":"2024-05-29T11:47:56.741331Z","shell.execute_reply.started":"2024-05-29T11:47:55.797508Z","shell.execute_reply":"2024-05-29T11:47:56.740149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\ndevice_name = tf.test.gpu_device_name()\nif \"GPU\" not in device_name:\n    print(\"GPU device not found\")\nprint('Found GPU at: {}'.format(device_name))","metadata":{"execution":{"iopub.status.busy":"2024-05-29T11:47:56.742579Z","iopub.execute_input":"2024-05-29T11:47:56.742913Z","iopub.status.idle":"2024-05-29T11:47:56.751434Z","shell.execute_reply.started":"2024-05-29T11:47:56.742886Z","shell.execute_reply":"2024-05-29T11:47:56.750533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nprint(tf.__version__)\nprint(tf.config.experimental.list_physical_devices('GPU'))\nprint(tf.test.gpu_device_name())","metadata":{"execution":{"iopub.status.busy":"2024-05-29T11:47:56.755826Z","iopub.execute_input":"2024-05-29T11:47:56.756096Z","iopub.status.idle":"2024-05-29T11:47:56.765840Z","shell.execute_reply.started":"2024-05-29T11:47:56.756073Z","shell.execute_reply":"2024-05-29T11:47:56.764811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tf.__version__)\nprint(tf.test.is_gpu_available())\nprint(tf.config.list_physical_devices('GPU'))","metadata":{"execution":{"iopub.status.busy":"2024-05-29T11:47:56.766904Z","iopub.execute_input":"2024-05-29T11:47:56.767190Z","iopub.status.idle":"2024-05-29T11:47:56.777566Z","shell.execute_reply.started":"2024-05-29T11:47:56.767167Z","shell.execute_reply":"2024-05-29T11:47:56.776729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python /kaggle/working/Tensorflow/models/research/object_detection/builders/model_builder_tf2_test.py","metadata":{"execution":{"iopub.status.busy":"2024-05-29T11:47:56.778525Z","iopub.execute_input":"2024-05-29T11:47:56.778828Z","iopub.status.idle":"2024-05-29T11:48:44.769864Z","shell.execute_reply.started":"2024-05-29T11:47:56.778797Z","shell.execute_reply":"2024-05-29T11:48:44.768579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)\nsession = tf.compat.v1.InteractiveSession(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))","metadata":{"execution":{"iopub.status.busy":"2024-05-29T11:48:44.771377Z","iopub.execute_input":"2024-05-29T11:48:44.771772Z","iopub.status.idle":"2024-05-29T11:48:44.782919Z","shell.execute_reply.started":"2024-05-29T11:48:44.771734Z","shell.execute_reply":"2024-05-29T11:48:44.781981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !python /kaggle/working/Tensorflow/models/research/object_detection/builders/model_builder_tf1_test.py","metadata":{"execution":{"iopub.status.busy":"2024-05-29T11:48:44.784168Z","iopub.execute_input":"2024-05-29T11:48:44.784882Z","iopub.status.idle":"2024-05-29T11:48:44.790311Z","shell.execute_reply.started":"2024-05-29T11:48:44.784811Z","shell.execute_reply":"2024-05-29T11:48:44.789503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"create a folder for pretrained model","metadata":{}},{"cell_type":"code","source":"!mkdir Tensorflow/pretrained","metadata":{"execution":{"iopub.status.busy":"2024-05-29T11:48:44.791433Z","iopub.execute_input":"2024-05-29T11:48:44.792037Z","iopub.status.idle":"2024-05-29T11:48:45.763052Z","shell.execute_reply.started":"2024-05-29T11:48:44.792007Z","shell.execute_reply":"2024-05-29T11:48:45.761735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %cd /kaggle/working/Tensorflow/pretrained\n# !wget http://download.tensorflow.org/models/object_detection/ssd_inception_v2_coco_2018_01_28.tar.gz\n# # Unzip\n# !tar -xzvf ssd_inception_v2_coco_2018_01_28.tar.gz","metadata":{"execution":{"iopub.status.busy":"2024-05-29T11:48:45.764806Z","iopub.execute_input":"2024-05-29T11:48:45.765722Z","iopub.status.idle":"2024-05-29T11:48:45.770260Z","shell.execute_reply.started":"2024-05-29T11:48:45.765680Z","shell.execute_reply":"2024-05-29T11:48:45.769157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"download pretrained model from api (there are a lot of ssd with many backbone: inception resnet, resnet, mobilenet, for more ìnformation check tensorflow github \nIn this project we choose ssd with backbone resnet v1 ","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working/Tensorflow/pretrained\n!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n# Unzip\n!tar -xzvf ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz","metadata":{"execution":{"iopub.status.busy":"2024-05-28T15:39:52.717056Z","iopub.execute_input":"2024-05-28T15:39:52.717337Z","iopub.status.idle":"2024-05-28T15:39:58.612243Z","shell.execute_reply.started":"2024-05-28T15:39:52.717309Z","shell.execute_reply":"2024-05-28T15:39:58.611315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.python.ops import control_flow_ops","metadata":{"execution":{"iopub.status.busy":"2024-05-28T15:39:58.613634Z","iopub.execute_input":"2024-05-28T15:39:58.613939Z","iopub.status.idle":"2024-05-28T15:39:58.618458Z","shell.execute_reply.started":"2024-05-28T15:39:58.613910Z","shell.execute_reply":"2024-05-28T15:39:58.617612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from tensorflow.python.client import device_lib\n# device_lib.list_local_devices()","metadata":{"execution":{"iopub.status.busy":"2024-05-28T15:39:58.619544Z","iopub.execute_input":"2024-05-28T15:39:58.619839Z","iopub.status.idle":"2024-05-28T15:40:00.324358Z","shell.execute_reply.started":"2024-05-28T15:39:58.619814Z","shell.execute_reply":"2024-05-28T15:40:00.323216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import tensorflow as tf\n# device_name = tf.test.gpu_device_name()\n# if \"GPU\" not in device_name:\n#     print(\"GPU device not found\")\n# print('Found GPU at: {}'.format(device_name))","metadata":{"execution":{"iopub.status.busy":"2024-05-28T15:40:00.325681Z","iopub.execute_input":"2024-05-28T15:40:00.326042Z","iopub.status.idle":"2024-05-28T15:40:00.333465Z","shell.execute_reply.started":"2024-05-28T15:40:00.326013Z","shell.execute_reply":"2024-05-28T15:40:00.332651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import tensorflow as tf\n# print(tf.__version__)\n# print(tf.config.experimental.list_physical_devices('GPU'))\n# print(tf.test.gpu_device_name())","metadata":{"execution":{"iopub.status.busy":"2024-05-28T15:40:00.334541Z","iopub.execute_input":"2024-05-28T15:40:00.334813Z","iopub.status.idle":"2024-05-28T15:40:00.343628Z","shell.execute_reply.started":"2024-05-28T15:40:00.334791Z","shell.execute_reply":"2024-05-28T15:40:00.342846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n\nprint(tf.test.is_built_with_cuda())","metadata":{"execution":{"iopub.status.busy":"2024-05-29T11:48:45.771502Z","iopub.execute_input":"2024-05-29T11:48:45.771894Z","iopub.status.idle":"2024-05-29T11:48:45.780450Z","shell.execute_reply.started":"2024-05-29T11:48:45.771868Z","shell.execute_reply":"2024-05-29T11:48:45.779410Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !nvcc --version","metadata":{"execution":{"iopub.status.busy":"2024-05-28T15:40:00.354981Z","iopub.execute_input":"2024-05-28T15:40:00.355244Z","iopub.status.idle":"2024-05-28T15:40:00.363892Z","shell.execute_reply.started":"2024-05-28T15:40:00.355221Z","shell.execute_reply":"2024-05-28T15:40:00.363035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import torch\n# print('CUDA:',torch.version.cuda)\n\n# cudnn = torch.backends.cudnn.version()\n# cudnn_major = cudnn // 1000\n# cudnn = cudnn % 1000\n# cudnn_minor = cudnn // 100\n# cudnn_patch = cudnn % 100\n# print( 'cuDNN:', '.'.join([str(cudnn_major),str(cudnn_minor),str(cudnn_patch)]) )","metadata":{"execution":{"iopub.status.busy":"2024-05-28T15:40:00.365828Z","iopub.execute_input":"2024-05-28T15:40:00.366106Z","iopub.status.idle":"2024-05-28T15:40:00.374092Z","shell.execute_reply.started":"2024-05-28T15:40:00.366082Z","shell.execute_reply":"2024-05-28T15:40:00.373318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install tensorflow-gpu==2.10.0","metadata":{"execution":{"iopub.status.busy":"2024-05-28T15:40:00.375455Z","iopub.execute_input":"2024-05-28T15:40:00.375699Z","iopub.status.idle":"2024-05-28T15:40:00.383077Z","shell.execute_reply.started":"2024-05-28T15:40:00.375678Z","shell.execute_reply":"2024-05-28T15:40:00.382309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !python /kaggle/working/Tensorflow/models/research/setup.py build\n# !python /kaggle/working/Tensorflow/models/research/setup.py install","metadata":{"execution":{"iopub.status.busy":"2024-05-28T15:40:00.388272Z","iopub.execute_input":"2024-05-28T15:40:00.388565Z","iopub.status.idle":"2024-05-28T15:40:00.393892Z","shell.execute_reply.started":"2024-05-28T15:40:00.388542Z","shell.execute_reply":"2024-05-28T15:40:00.393097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# config = '''\n# model {\n#   ssd {\n#     num_classes: 19\n#     image_resizer {\n#       fixed_shape_resizer {\n#         height: 320\n#         width: 320\n#       }\n#     }\n#     feature_extractor {\n#       type: \"ssd_mobilenet_v2_fpn_keras\"\n#       depth_multiplier: 1.0\n#       min_depth: 16\n#       conv_hyperparams {\n#         regularizer {\n#           l2_regularizer {\n#             weight: 3.9999998989515007e-05\n#           }\n#         }\n#         initializer {\n#           random_normal_initializer {\n#             mean: 0.0\n#             stddev: 0.009999999776482582\n#           }\n#         }\n#         activation: RELU_6\n#         batch_norm {\n#           decay: 0.996999979019165\n#           scale: true\n#           epsilon: 0.0010000000474974513\n#         }\n#       }\n#       use_depthwise: true\n#       override_base_feature_extractor_hyperparams: true\n#       fpn {\n#         min_level: 3\n#         max_level: 7\n#         additional_layer_depth: 128\n#       }\n#     }\n#     box_coder {\n#       faster_rcnn_box_coder {\n#         y_scale: 10.0\n#         x_scale: 10.0\n#         height_scale: 5.0\n#         width_scale: 5.0\n#       }\n#     }\n#     matcher {\n#       argmax_matcher {\n#         matched_threshold: 0.5\n#         unmatched_threshold: 0.5\n#         ignore_thresholds: false\n#         negatives_lower_than_unmatched: true\n#         force_match_for_each_row: true\n#         use_matmul_gather: true\n#       }\n#     }\n#     similarity_calculator {\n#       iou_similarity {\n#       }\n#     }\n#     box_predictor {\n#       weight_shared_convolutional_box_predictor {\n#         conv_hyperparams {\n#           regularizer {\n#             l2_regularizer {\n#               weight: 3.9999998989515007e-05\n#             }\n#           }\n#           initializer {\n#             random_normal_initializer {\n#               mean: 0.0\n#               stddev: 0.009999999776482582\n#             }\n#           }\n#           activation: RELU_6\n#           batch_norm {\n#             decay: 0.996999979019165\n#             scale: true\n#             epsilon: 0.0010000000474974513\n#           }\n#         }\n#         depth: 128\n#         num_layers_before_predictor: 4\n#         kernel_size: 3\n#         class_prediction_bias_init: -4.599999904632568\n#         share_prediction_tower: true\n#         use_depthwise: true\n#       }\n#     }\n#     anchor_generator {\n#       multiscale_anchor_generator {\n#         min_level: 3\n#         max_level: 7\n#         anchor_scale: 4.0\n#         aspect_ratios: 1.0\n#         aspect_ratios: 2.0\n#         aspect_ratios: 0.5\n#         scales_per_octave: 2\n#       }\n#     }\n#     post_processing {\n#       batch_non_max_suppression {\n#         score_threshold: 9.99999993922529e-09\n#         iou_threshold: 0.6000000238418579\n#         max_detections_per_class: 100\n#         max_total_detections: 100\n#         use_static_shapes: false\n#       }\n#       score_converter: SIGMOID\n#     }\n#     normalize_loss_by_num_matches: true\n#     loss {\n#       localization_loss {\n#         weighted_smooth_l1 {\n#         }\n#       }\n#       classification_loss {\n#         weighted_sigmoid_focal {\n#           gamma: 2.0\n#           alpha: 0.25\n#         }\n#       }\n#       classification_weight: 1.0\n#       localization_weight: 1.0\n#     }\n#     encode_background_as_zeros: true\n#     normalize_loc_loss_by_codesize: true\n#     inplace_batchnorm_update: true\n#     freeze_batchnorm: false\n#   }\n# }\n# train_config {\n#   batch_size: 24\n#   data_augmentation_options {\n#     random_horizontal_flip {\n#     }\n#   }\n#   data_augmentation_options {\n#     random_crop_image {\n#       min_object_covered: 0.0\n#       min_aspect_ratio: 0.75\n#       max_aspect_ratio: 3.0\n#       min_area: 0.75\n#       max_area: 1.0\n#       overlap_thresh: 0.0\n#     }\n#   }\n#   sync_replicas: true\n#   optimizer {\n#     momentum_optimizer {\n#       learning_rate {\n#         cosine_decay_learning_rate {\n#           learning_rate_base: 0.07999999821186066\n#           total_steps: 50000\n#           warmup_learning_rate: 0.026666000485420227\n#           warmup_steps: 1000\n#         }\n#       }\n#       momentum_optimizer_value: 0.8999999761581421\n#     }\n#     use_moving_average: false\n#   }\n#   fine_tune_checkpoint: \"/kaggle/working/Tensorflow/pretrained/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint/ckpt-0\"\n#   num_steps: 50000\n#   startup_delay_steps: 0.0\n#   replicas_to_aggregate: 8\n#   max_number_of_boxes: 100\n#   unpad_groundtruth_tensors: false\n#   fine_tune_checkpoint_type: \"detection\"\n#   fine_tune_checkpoint_version: V2\n# }\n# train_input_reader {\n#   label_map_path: \"/kaggle/working/Tensorflow/PokemonDetection_SSD/label_map.txt\"\n#   tf_record_input_reader {\n#     input_path: \"/kaggle/working/Tensorflow/PokemonDetection_SSD/tfrecord_data/train.record\"\n#   }\n# }\n# eval_config {\n#   metrics_set: \"coco_detection_metrics\"\n#   use_moving_averages: false\n# }\n# eval_input_reader {\n#   label_map_path: \"/kaggle/working/Tensorflow/PokemonDetection_SSD/label_map.txt\"\n#   shuffle: false\n#   num_epochs: 1\n#   tf_record_input_reader {\n#     input_path: \"/kaggle/working/Tensorflow/PokemonDetection_SSD/tfrecord_data/test.record\"\n#   }\n# }\n\n# '''","metadata":{"execution":{"iopub.status.busy":"2024-05-28T15:40:00.395190Z","iopub.execute_input":"2024-05-28T15:40:00.395463Z","iopub.status.idle":"2024-05-28T15:40:00.406554Z","shell.execute_reply.started":"2024-05-28T15:40:00.395439Z","shell.execute_reply":"2024-05-28T15:40:00.405703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Change 'num_class' to number of your class \n* Change fine_tune_checkpoint: \"/kaggle/working/Tensorflow/pretrained/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/checkpoint/ckpt-0\" \n* In train_input_reader \n* Change label_map_path: \"/kaggle/working/Tensorflow/PokemonDetection_SSD/label_map.txt\"\n* Change input_path: \"/kaggle/working/Tensorflow/PokemonDetection_SSD/tfrecord_data/train.record\"\n* In eval_input_reader\n* Change label_map_path: \"/kaggle/working/Tensorflow/PokemonDetection_SSD/label_map.txt\"\n* Change input_path: \"/kaggle/working/Tensorflow/PokemonDetection_SSD/tfrecord_data/test.record\"\n* And change fine_tune_checkpoint_type: \"full\" -> fine_tune_checkpoint_type: \"detection\" (from 'full' to 'detection') ","metadata":{}},{"cell_type":"code","source":"config = '''\nmodel {\n  ssd {\n    num_classes: 19\n    image_resizer {\n      fixed_shape_resizer {\n        height: 640\n        width: 640\n      }\n    }\n    feature_extractor {\n      type: \"ssd_resnet50_v1_fpn_keras\"\n      depth_multiplier: 1.0\n      min_depth: 16\n      conv_hyperparams {\n        regularizer {\n          l2_regularizer {\n            weight: 0.00039999998989515007\n          }\n        }\n        initializer {\n          truncated_normal_initializer {\n            mean: 0.0\n            stddev: 0.029999999329447746\n          }\n        }\n        activation: RELU_6\n        batch_norm {\n          decay: 0.996999979019165\n          scale: true\n          epsilon: 0.0010000000474974513\n        }\n      }\n      override_base_feature_extractor_hyperparams: true\n      fpn {\n        min_level: 3\n        max_level: 7\n      }\n    }\n    box_coder {\n      faster_rcnn_box_coder {\n        y_scale: 10.0\n        x_scale: 10.0\n        height_scale: 5.0\n        width_scale: 5.0\n      }\n    }\n    matcher {\n      argmax_matcher {\n        matched_threshold: 0.5\n        unmatched_threshold: 0.5\n        ignore_thresholds: false\n        negatives_lower_than_unmatched: true\n        force_match_for_each_row: true\n        use_matmul_gather: true\n      }\n    }\n    similarity_calculator {\n      iou_similarity {\n      }\n    }\n    box_predictor {\n      weight_shared_convolutional_box_predictor {\n        conv_hyperparams {\n          regularizer {\n            l2_regularizer {\n              weight: 0.00039999998989515007\n            }\n          }\n          initializer {\n            random_normal_initializer {\n              mean: 0.0\n              stddev: 0.009999999776482582\n            }\n          }\n          activation: RELU_6\n          batch_norm {\n            decay: 0.996999979019165\n            scale: true\n            epsilon: 0.0010000000474974513\n          }\n        }\n        depth: 256\n        num_layers_before_predictor: 4\n        kernel_size: 3\n        class_prediction_bias_init: -4.599999904632568\n      }\n    }\n    anchor_generator {\n      multiscale_anchor_generator {\n        min_level: 3\n        max_level: 7\n        anchor_scale: 4.0\n        aspect_ratios: 1.0\n        aspect_ratios: 2.0\n        aspect_ratios: 0.5\n        scales_per_octave: 2\n      }\n    }\n    post_processing {\n      batch_non_max_suppression {\n        score_threshold: 9.99999993922529e-09\n        iou_threshold: 0.6000000238418579\n        max_detections_per_class: 100\n        max_total_detections: 100\n        use_static_shapes: false\n      }\n      score_converter: SIGMOID\n    }\n    normalize_loss_by_num_matches: true\n    loss {\n      localization_loss {\n        weighted_smooth_l1 {\n        }\n      }\n      classification_loss {\n        weighted_sigmoid_focal {\n          gamma: 2.0\n          alpha: 0.25\n        }\n      }\n      classification_weight: 1.0\n      localization_weight: 1.0\n    }\n    encode_background_as_zeros: true\n    normalize_loc_loss_by_codesize: true\n    inplace_batchnorm_update: true\n    freeze_batchnorm: false\n  }\n}\ntrain_config {\n  batch_size: 22\n  data_augmentation_options {\n    random_horizontal_flip {\n    }\n  }\n  data_augmentation_options {\n    random_crop_image {\n      min_object_covered: 0.0\n      min_aspect_ratio: 0.75\n      max_aspect_ratio: 3.0\n      min_area: 0.75\n      max_area: 1.0\n      overlap_thresh: 0.0\n    }\n  }\n  sync_replicas: true\n  optimizer {\n    momentum_optimizer {\n      learning_rate {\n        cosine_decay_learning_rate {\n          learning_rate_base: 0.03999999910593033\n          total_steps: 25000\n          warmup_learning_rate: 0.013333000242710114\n          warmup_steps: 2000\n        }\n      }\n      momentum_optimizer_value: 0.8999999761581421\n    }\n    use_moving_average: false\n  }\n  fine_tune_checkpoint: \"/kaggle/working/Tensorflow/pretrained/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/checkpoint/ckpt-0\"\n  num_steps: 25000\n  startup_delay_steps: 0.0\n  replicas_to_aggregate: 8\n  max_number_of_boxes: 100\n  unpad_groundtruth_tensors: false\n  fine_tune_checkpoint_type: \"detection\"\n  use_bfloat16: true\n  fine_tune_checkpoint_version: V2\n}\ntrain_input_reader {\n  label_map_path: \"/kaggle/working/Tensorflow/PokemonDetection_SSD/label_map.txt\"\n  tf_record_input_reader {\n    input_path: \"/kaggle/working/Tensorflow/PokemonDetection_SSD/tfrecord_data/train.record\"\n  }\n}\neval_config {\n  metrics_set: \"coco_detection_metrics\"\n  use_moving_averages: false\n}\neval_input_reader {\n  label_map_path: \"/kaggle/working/Tensorflow/PokemonDetection_SSD/label_map.txt\"\n  shuffle: false\n  num_epochs: 1\n  tf_record_input_reader {\n    input_path: \"/kaggle/working/Tensorflow/PokemonDetection_SSD/tfrecord_data/test.record\"\n  }\n}\n'''","metadata":{"execution":{"iopub.status.busy":"2024-05-28T15:57:57.202610Z","iopub.execute_input":"2024-05-28T15:57:57.202887Z","iopub.status.idle":"2024-05-28T15:57:57.214104Z","shell.execute_reply.started":"2024-05-28T15:57:57.202864Z","shell.execute_reply":"2024-05-28T15:57:57.213142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/working/Tensorflow/pretrained/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/pipeline.config', 'w') as fp:\n    fp.write(config)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T15:57:57.231875Z","iopub.execute_input":"2024-05-28T15:57:57.232232Z","iopub.status.idle":"2024-05-28T15:57:58.056910Z","shell.execute_reply.started":"2024-05-28T15:57:57.232200Z","shell.execute_reply":"2024-05-28T15:57:58.055410Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"create folder for model when training","metadata":{}},{"cell_type":"code","source":"!mkdir /kaggle/working/Tensorflow/output_model","metadata":{"execution":{"iopub.status.busy":"2024-05-28T15:57:58.057770Z","iopub.status.idle":"2024-05-28T15:57:58.058123Z","shell.execute_reply.started":"2024-05-28T15:57:58.057953Z","shell.execute_reply":"2024-05-28T15:57:58.057967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Change label map ","metadata":{}},{"cell_type":"code","source":"label_map = '''\nitem { name:'Cinderace' id:1}\nitem { name:'Dracovish' id:2}\nitem { name:'Dragonite' id:3}\nitem { name:'Eevee' id:4,}\nitem { name:'Eternatus' id:5}\nitem { name:'Gengar' id:6}\nitem { name:'Grookey' id:7}\nitem { name:'Inteleon' id:8}\nitem { name:'Lucario' id:9}\nitem { name:'Meowth' id:10}\nitem { name:'Mew' id:11}\nitem { name:'Mimey' id:12}\nitem { name:'Morpeko' id:13}\nitem { name:'Pikachu' id:14}\nitem { name:'Sirfetchd' id:15}\nitem { name:'Wobbuffet' id:16}\nitem { name:'Yamper' id:17}\nitem { name:'Zacian' id:18}\nitem { name:'Zamazenta' id:19}\n'''","metadata":{"execution":{"iopub.status.busy":"2024-05-29T11:48:45.781520Z","iopub.execute_input":"2024-05-29T11:48:45.781816Z","iopub.status.idle":"2024-05-29T11:48:45.788711Z","shell.execute_reply.started":"2024-05-29T11:48:45.781791Z","shell.execute_reply":"2024-05-29T11:48:45.787791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/working/Tensorflow/PokemonDetection_SSD/label_map.txt', 'w') as fp:\n    fp.write(label_map)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T11:48:45.789882Z","iopub.execute_input":"2024-05-29T11:48:45.790356Z","iopub.status.idle":"2024-05-29T11:48:45.800526Z","shell.execute_reply.started":"2024-05-29T11:48:45.790326Z","shell.execute_reply":"2024-05-29T11:48:45.799753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"An error will appear ","metadata":{}},{"cell_type":"code","source":"# !python /kaggle/working/Tensorflow/models/research/object_detection/model_main_tf2.py  --num_worker = 2 \\\n# --pipeline_config_path=/kaggle/working/Tensorflow/pretrained/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/pipeline.config \\\n# --model_dir=/kaggle/working/Tensorflow/output_model --num_train_steps=100000 ","metadata":{"execution":{"iopub.status.busy":"2024-05-23T14:31:29.850200Z","iopub.execute_input":"2024-05-23T14:31:29.850590Z","iopub.status.idle":"2024-05-23T14:31:41.920141Z","shell.execute_reply.started":"2024-05-23T14:31:29.850562Z","shell.execute_reply":"2024-05-23T14:31:41.919105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Do this step to fix the error","metadata":{}},{"cell_type":"code","source":"# !cat /opt/conda/lib/python3.10/site-packages/tf_slim/data/tfexample_decoder.py","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fix_bug_to_use_gpu = '''\n# coding=utf-8\n# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Contains the TFExampleDecoder its associated helper classes.\n\nThe TFExampleDecode is a DataDecoder used to decode TensorFlow Example protos.\nIn order to do so each requested item must be paired with one or more Example\nfeatures that are parsed to produce the Tensor-based manifestation of the item.\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\n\nimport six\nfrom tf_slim.data import data_decoder\n# pylint:disable=g-direct-tensorflow-import\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import sparse_tensor\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import array_ops_stack\nfrom tensorflow.python.ops import check_ops\nfrom tensorflow.python.ops import cond\nfrom tensorflow.python.ops import control_flow_case\nfrom tensorflow.python.ops import image_ops\nfrom tensorflow.python.ops import map_fn\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import parsing_ops\nfrom tensorflow.python.ops import sparse_ops\n# pylint:enable=g-direct-tensorflow-import\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass ItemHandler(object):\n  \"\"\"Specifies the item-to-Features mapping for tf.parse_example.\n\n  An ItemHandler both specifies a list of Features used for parsing an Example\n  proto as well as a function that post-processes the results of Example\n  parsing.\n  \"\"\"\n\n  def __init__(self, keys):\n    \"\"\"Constructs the handler with the name of the tf.train.Feature keys to use.\n\n    Args:\n      keys: the name of the TensorFlow Example Feature.\n    \"\"\"\n    if not isinstance(keys, (tuple, list)):\n      keys = [keys]\n    self._keys = keys\n\n  @property\n  def keys(self):\n    return self._keys\n\n  @abc.abstractmethod\n  def tensors_to_item(self, keys_to_tensors):\n    \"\"\"Maps the given dictionary of tensors to the requested item.\n\n    Args:\n      keys_to_tensors: a mapping of TF-Example keys to parsed tensors.\n\n    Returns:\n      the final tensor representing the item being handled.\n    \"\"\"\n    pass\n\n\nclass ItemHandlerCallback(ItemHandler):\n  \"\"\"An ItemHandler that converts the parsed tensors via a given function.\n\n  Unlike other ItemHandlers, the ItemHandlerCallback resolves its item via\n  a callback function rather than using prespecified behavior.\n  \"\"\"\n\n  def __init__(self, keys, func):\n    \"\"\"Initializes the ItemHandler.\n\n    Args:\n      keys: a list of TF-Example keys.\n      func: a function that takes as an argument a dictionary from `keys` to\n        parsed Tensors.\n    \"\"\"\n    super(ItemHandlerCallback, self).__init__(keys)\n    self._func = func\n\n  def tensors_to_item(self, keys_to_tensors):\n    return self._func(keys_to_tensors)\n\n\nclass BoundingBox(ItemHandler):\n  \"\"\"An ItemHandler that concatenates a set of parsed Tensors to Bounding Boxes.\n  \"\"\"\n\n  def __init__(self, keys=None, prefix=''):\n    \"\"\"Initialize the bounding box handler.\n\n    Args:\n      keys: A list of four key names representing the ymin, xmin, ymax, mmax\n      prefix: An optional prefix for each of the bounding box keys.\n        If provided, `prefix` is appended to each key in `keys`.\n\n    Raises:\n      ValueError: if keys is not `None` and also not a list of exactly 4 keys\n    \"\"\"\n    if keys is None:\n      keys = ['ymin', 'xmin', 'ymax', 'xmax']\n    elif len(keys) != 4:\n      raise ValueError('BoundingBox expects 4 keys but got {}'.format(\n          len(keys)))\n    self._prefix = prefix\n    self._keys = keys\n    self._full_keys = [prefix + k for k in keys]\n    super(BoundingBox, self).__init__(self._full_keys)\n\n  def tensors_to_item(self, keys_to_tensors):\n    \"\"\"Maps the given dictionary of tensors to a concatenated list of bboxes.\n\n    Args:\n      keys_to_tensors: a mapping of TF-Example keys to parsed tensors.\n\n    Returns:\n      [num_boxes, 4] tensor of bounding box coordinates,\n        i.e. 1 bounding box per row, in order [y_min, x_min, y_max, x_max].\n    \"\"\"\n    sides = []\n    for key in self._full_keys:\n      side = keys_to_tensors[key]\n      if isinstance(side, sparse_tensor.SparseTensor):\n        side = side.values\n      side = array_ops.expand_dims(side, 0)\n      sides.append(side)\n\n    bounding_box = array_ops.concat(sides, 0)\n    return array_ops.transpose(bounding_box)\n\n\nclass Tensor(ItemHandler):\n  \"\"\"An ItemHandler that returns a parsed Tensor.\"\"\"\n\n  def __init__(self, tensor_key, shape_keys=None, shape=None, default_value=0):\n    \"\"\"Initializes the Tensor handler.\n\n    Tensors are, by default, returned without any reshaping. However, there are\n    two mechanisms which allow reshaping to occur at load time. If `shape_keys`\n    is provided, both the `Tensor` corresponding to `tensor_key` and\n    `shape_keys` is loaded and the former `Tensor` is reshaped with the values\n    of the latter. Alternatively, if a fixed `shape` is provided, the `Tensor`\n    corresponding to `tensor_key` is loaded and reshape appropriately.\n    If neither `shape_keys` nor `shape` are provided, the `Tensor` will be\n    returned without any reshaping.\n\n    Args:\n      tensor_key: the name of the `TFExample` feature to read the tensor from.\n      shape_keys: Optional name or list of names of the TF-Example feature in\n        which the tensor shape is stored. If a list, then each corresponds to\n        one dimension of the shape.\n      shape: Optional output shape of the `Tensor`. If provided, the `Tensor` is\n        reshaped accordingly.\n      default_value: The value used when the `tensor_key` is not found in a\n        particular `TFExample`.\n\n    Raises:\n      ValueError: if both `shape_keys` and `shape` are specified.\n    \"\"\"\n    if shape_keys and shape is not None:\n      raise ValueError('Cannot specify both shape_keys and shape parameters.')\n    if shape_keys and not isinstance(shape_keys, list):\n      shape_keys = [shape_keys]\n    self._tensor_key = tensor_key\n    self._shape_keys = shape_keys\n    self._shape = shape\n    self._default_value = default_value\n    keys = [tensor_key]\n    if shape_keys:\n      keys.extend(shape_keys)\n    super(Tensor, self).__init__(keys)\n\n  def tensors_to_item(self, keys_to_tensors):\n    tensor = keys_to_tensors[self._tensor_key]\n    shape = self._shape\n    if self._shape_keys:\n      shape_dims = []\n      for k in self._shape_keys:\n        shape_dim = keys_to_tensors[k]\n        if isinstance(shape_dim, sparse_tensor.SparseTensor):\n          shape_dim = sparse_ops.sparse_tensor_to_dense(shape_dim)\n        shape_dims.append(shape_dim)\n      shape = array_ops.reshape(array_ops_stack.stack(shape_dims), [-1])\n    if isinstance(tensor, sparse_tensor.SparseTensor):\n      if shape is not None:\n        tensor = sparse_ops.sparse_reshape(tensor, shape)\n      tensor = sparse_ops.sparse_tensor_to_dense(tensor, self._default_value)\n    else:\n      if shape is not None:\n        tensor = array_ops.reshape(tensor, shape)\n    return tensor\n\n\nclass LookupTensor(Tensor):\n  \"\"\"An ItemHandler that returns a parsed Tensor, the result of a lookup.\"\"\"\n\n  def __init__(self,\n               tensor_key,\n               table,\n               shape_keys=None,\n               shape=None,\n               default_value=''):\n    \"\"\"Initializes the LookupTensor handler.\n\n    See Tensor.  Simply calls a vocabulary (most often, a label mapping) lookup.\n\n    Args:\n      tensor_key: the name of the `TFExample` feature to read the tensor from.\n      table: A tf.lookup table.\n      shape_keys: Optional name or list of names of the TF-Example feature in\n        which the tensor shape is stored. If a list, then each corresponds to\n        one dimension of the shape.\n      shape: Optional output shape of the `Tensor`. If provided, the `Tensor` is\n        reshaped accordingly.\n      default_value: The value used when the `tensor_key` is not found in a\n        particular `TFExample`.\n\n    Raises:\n      ValueError: if both `shape_keys` and `shape` are specified.\n    \"\"\"\n    self._table = table\n    super(LookupTensor, self).__init__(tensor_key, shape_keys, shape,\n                                       default_value)\n\n  def tensors_to_item(self, keys_to_tensors):\n    unmapped_tensor = super(LookupTensor, self).tensors_to_item(keys_to_tensors)\n    return self._table.lookup(unmapped_tensor)\n\n\nclass BackupHandler(ItemHandler):\n  \"\"\"An ItemHandler that tries two ItemHandlers in order.\"\"\"\n\n  def __init__(self, handler, backup):\n    \"\"\"Initializes the BackupHandler handler.\n\n    If the first Handler's tensors_to_item returns a Tensor with no elements,\n    the second Handler is used.\n\n    Args:\n      handler: The primary ItemHandler.\n      backup: The backup ItemHandler.\n\n    Raises:\n      ValueError: if either is not an ItemHandler.\n    \"\"\"\n    if not isinstance(handler, ItemHandler):\n      raise ValueError('Primary handler is of type %s instead of ItemHandler'\n                       % type(handler))\n    if not isinstance(backup, ItemHandler):\n      raise ValueError('Backup handler is of type %s instead of ItemHandler'\n                       % type(backup))\n    self._handler = handler\n    self._backup = backup\n    super(BackupHandler, self).__init__(handler.keys + backup.keys)\n\n  def tensors_to_item(self, keys_to_tensors):\n    item = self._handler.tensors_to_item(keys_to_tensors)\n    return cond.cond(\n        pred=math_ops.equal(math_ops.reduce_prod(array_ops.shape(item)), 0),\n        true_fn=lambda: self._backup.tensors_to_item(keys_to_tensors),\n        false_fn=lambda: item)\n\n\nclass SparseTensor(ItemHandler):\n  \"\"\"An ItemHandler for SparseTensors.\"\"\"\n\n  def __init__(self,\n               indices_key=None,\n               values_key=None,\n               shape_key=None,\n               shape=None,\n               densify=False,\n               default_value=0):\n    \"\"\"Initializes the Tensor handler.\n\n    Args:\n      indices_key: the name of the TF-Example feature that contains the ids.\n        Defaults to 'indices'.\n      values_key: the name of the TF-Example feature that contains the values.\n        Defaults to 'values'.\n      shape_key: the name of the TF-Example feature that contains the shape.\n        If provided it would be used.\n      shape: the output shape of the SparseTensor. If `shape_key` is not\n        provided this `shape` would be used.\n      densify: whether to convert the SparseTensor into a dense Tensor.\n      default_value: Scalar value to set when making dense for indices not\n        specified in the `SparseTensor`.\n    \"\"\"\n    indices_key = indices_key or 'indices'\n    values_key = values_key or 'values'\n    self._indices_key = indices_key\n    self._values_key = values_key\n    self._shape_key = shape_key\n    self._shape = shape\n    self._densify = densify\n    self._default_value = default_value\n    keys = [indices_key, values_key]\n    if shape_key:\n      keys.append(shape_key)\n    super(SparseTensor, self).__init__(keys)\n\n  def tensors_to_item(self, keys_to_tensors):\n    indices = keys_to_tensors[self._indices_key]\n    values = keys_to_tensors[self._values_key]\n    if self._shape_key:\n      shape = keys_to_tensors[self._shape_key]\n      if isinstance(shape, sparse_tensor.SparseTensor):\n        shape = sparse_ops.sparse_tensor_to_dense(shape)\n    elif self._shape:\n      shape = self._shape\n    else:\n      shape = indices.dense_shape\n    indices_shape = array_ops.shape(indices.indices)\n    rank = indices_shape[1]\n    ids = math_ops.cast(indices.values, dtypes.int64)\n    indices_columns_to_preserve = array_ops.slice(\n        indices.indices, [0, 0], array_ops_stack.stack([-1, rank - 1]))\n    new_indices = array_ops.concat(\n        [indices_columns_to_preserve, array_ops.reshape(ids, [-1, 1])], 1)\n\n    tensor = sparse_tensor.SparseTensor(new_indices, values.values, shape)\n    if self._densify:\n      tensor = sparse_ops.sparse_tensor_to_dense(tensor, self._default_value)\n    return tensor\n\n\nclass Image(ItemHandler):\n  \"\"\"An ItemHandler that decodes a parsed Tensor as an image.\"\"\"\n\n  def __init__(self,\n               image_key=None,\n               format_key=None,\n               shape=None,\n               channels=3,\n               dtype=dtypes.uint8,\n               repeated=False,\n               dct_method=''):\n    \"\"\"Initializes the image.\n\n    Args:\n      image_key: the name of the TF-Example feature in which the encoded image\n        is stored.\n      format_key: the name of the TF-Example feature in which the image format\n        is stored.\n      shape: the output shape of the image as 1-D `Tensor`\n        [height, width, channels]. If provided, the image is reshaped\n        accordingly. If left as None, no reshaping is done. A shape should\n        be supplied only if all the stored images have the same shape.\n      channels: the number of channels in the image.\n      dtype: images will be decoded at this bit depth. Different formats\n        support different bit depths.\n          See tf.image.decode_image,\n              tf.io.decode_raw,\n      repeated: if False, decodes a single image. If True, decodes a\n        variable number of image strings from a 1D tensor of strings.\n      dct_method: An optional string. Defaults to empty string. It only takes\n        effect when image format is jpeg, used to specify a hint about the\n        algorithm used for jpeg decompression. Currently valid values\n        are ['INTEGER_FAST', 'INTEGER_ACCURATE']. The hint may be ignored, for\n        example, the jpeg library does not have that specific option.\n    \"\"\"\n    if not image_key:\n      image_key = 'image/encoded'\n    if not format_key:\n      format_key = 'image/format'\n\n    super(Image, self).__init__([image_key, format_key])\n    self._image_key = image_key\n    self._format_key = format_key\n    self._shape = shape\n    self._channels = channels\n    self._dtype = dtype\n    self._repeated = repeated\n    self._dct_method = dct_method\n\n  def tensors_to_item(self, keys_to_tensors):\n    \"\"\"See base class.\"\"\"\n    image_buffer = keys_to_tensors[self._image_key]\n    image_format = keys_to_tensors[self._format_key]\n\n    if self._repeated:\n      return map_fn.map_fn(lambda x: self._decode(x, image_format),\n                           image_buffer, dtype=self._dtype)\n    else:\n      return self._decode(image_buffer, image_format)\n\n  def _decode(self, image_buffer, image_format):\n    \"\"\"Decodes the image buffer.\n\n    Args:\n      image_buffer: The tensor representing the encoded image tensor.\n      image_format: The image format for the image in `image_buffer`. If image\n        format is `raw`, all images are expected to be in this format, otherwise\n        this op can decode a mix of `jpg` and `png` formats.\n\n    Returns:\n      A tensor that represents decoded image of self._shape, or\n      (?, ?, self._channels) if self._shape is not specified.\n    \"\"\"\n\n    def decode_image():\n      \"\"\"Decodes a image based on the headers.\"\"\"\n      return math_ops.cast(\n          image_ops.decode_image(image_buffer, channels=self._channels),\n          self._dtype)\n\n    def decode_jpeg():\n      \"\"\"Decodes a jpeg image with specified '_dct_method'.\"\"\"\n      return math_ops.cast(\n          image_ops.decode_jpeg(\n              image_buffer,\n              channels=self._channels,\n              dct_method=self._dct_method), self._dtype)\n\n    def check_jpeg():\n      \"\"\"Checks if an image is jpeg.\"\"\"\n      # For jpeg, we directly use image_ops.decode_jpeg rather than decode_image\n      # in order to feed the jpeg specify parameter 'dct_method'.\n      return cond.cond(\n          image_ops.is_jpeg(image_buffer),\n          decode_jpeg,\n          decode_image,\n          name='cond_jpeg')\n\n    def decode_raw():\n      \"\"\"Decodes a raw image.\"\"\"\n      return parsing_ops.decode_raw(image_buffer, out_type=self._dtype)\n\n    pred_fn_pairs = [(math_ops.logical_or(\n        math_ops.equal(image_format, 'raw'),\n        math_ops.equal(image_format, 'RAW')), decode_raw)]\n\n    image = control_flow_case.case(\n        pred_fn_pairs, default=check_jpeg, exclusive=True)\n\n    image.set_shape([None, None, self._channels])\n    if self._shape is not None:\n      image = array_ops.reshape(image, self._shape)\n\n    return image\n\n\nclass BoundingBoxSequence(ItemHandler):\n  \"\"\"An ItemHandler that concatenates SparseTensors to Bounding Boxes.\n  \"\"\"\n\n  def __init__(self, keys=None, prefix=None, return_dense=True,\n               default_value=-1.0):\n    \"\"\"Initialize the bounding box handler.\n\n    Args:\n      keys: A list of four key names representing the ymin, xmin, ymax, xmax\n        in the Example or SequenceExample.\n      prefix: An optional prefix for each of the bounding box keys in the\n        Example or SequenceExample. If provided, `prefix` is prepended to each\n        key in `keys`.\n      return_dense: if True, returns a dense tensor; if False, returns as\n        sparse tensor.\n      default_value: The value used when the `tensor_key` is not found in a\n        particular `TFExample`.\n\n    Raises:\n      ValueError: if keys is not `None` and also not a list of exactly 4 keys\n    \"\"\"\n    if keys is None:\n      keys = ['ymin', 'xmin', 'ymax', 'xmax']\n    elif len(keys) != 4:\n      raise ValueError('BoundingBoxSequence expects 4 keys but got {}'.format(\n          len(keys)))\n    self._prefix = prefix\n    self._keys = keys\n    self._full_keys = [prefix + k for k in keys]\n    self._return_dense = return_dense\n    self._default_value = default_value\n    super(BoundingBoxSequence, self).__init__(self._full_keys)\n\n  def tensors_to_item(self, keys_to_tensors):\n    \"\"\"Maps the given dictionary of tensors to a concatenated list of bboxes.\n\n    Args:\n      keys_to_tensors: a mapping of TF-Example keys to parsed tensors.\n\n    Returns:\n      [time, num_boxes, 4] tensor of bounding box coordinates, in order\n          [y_min, x_min, y_max, x_max]. Whether the tensor is a SparseTensor\n          or a dense Tensor is determined by the return_dense parameter. Empty\n          positions in the sparse tensor are filled with -1.0 values.\n    \"\"\"\n    sides = []\n    for key in self._full_keys:\n      value = keys_to_tensors[key]\n      expanded_dims = array_ops.concat(\n          [math_ops.to_int64(array_ops.shape(value)),\n           constant_op.constant([1], dtype=dtypes.int64)], 0)\n      side = sparse_ops.sparse_reshape(value, expanded_dims)\n      sides.append(side)\n    bounding_boxes = sparse_ops.sparse_concat(2, sides)\n    if self._return_dense:\n      bounding_boxes = sparse_ops.sparse_tensor_to_dense(\n          bounding_boxes, default_value=self._default_value)\n    return bounding_boxes\n\n\nclass NumBoxesSequence(ItemHandler):\n  \"\"\"An ItemHandler that returns num_boxes per frame for a box sequence.\n\n  `num_boxes` is inferred from a 2D SparseTensor decoded from a field in the\n  SequenceExample. The SparseTensor is partially dense and only ragged along its\n  second dimensions.\n\n  The output is an int64 tf.Tensor of shape [time], which is solely determined\n  by the tensor of the first key. However, if `check_consistency` is True, this\n  function checks that `num_boxes` is consistent across all keys.\n  \"\"\"\n\n  def __init__(self, keys=None, check_consistency=True):\n    \"\"\"Initialization.\n\n    Args:\n      keys: A list of keys of sparse tensors which have exactly 2 dimensions,\n        with the 1st being the `time` and the 2nd the `boxes` per frame.\n        key in `keys`.\n      check_consistency: if True, check for consistency.\n\n    Raises:\n      ValueError: If keys is empty.\n    \"\"\"\n    if not keys:\n      raise ValueError('keys must not be empty.')\n    self._check_consistency = check_consistency\n    super(NumBoxesSequence, self).__init__(keys)\n\n  def tensors_to_item(self, keys_to_tensors):\n    \"\"\"Maps the given dictionary of tensors to a num_boxes tensor.\n\n    If check_consistency is True: raises runtime error in Tensorflow when the\n    consistency is violated across tensors.\n\n    Args:\n      keys_to_tensors: A mapping of TF-Example keys to parsed tensors.\n\n    Returns:\n      [time] tf.Tensor containing the number of boxes per frame.\n\n    Raises:\n      ValueError: If any of the keyed tensors is not sparse or exactly 2\n        dimensional.\n    \"\"\"\n    def _compute_num_boxes(tensor):\n      \"\"\"Compute num_boxes from a single 2D tensor.\"\"\"\n      if not isinstance(tensor, sparse_tensor.SparseTensor):\n        raise ValueError('tensor must be of type tf.SparseTensor.')\n      indices = tensor.indices\n      dense_shape = tensor.dense_shape\n      box_ids = indices[:, 1]\n      box_ids = sparse_tensor.SparseTensor(\n          indices=indices, values=box_ids, dense_shape=dense_shape)\n      box_ids = sparse_ops.sparse_tensor_to_dense(box_ids, default_value=-1)\n      # In the event that the parsed tensor is empty (perhaps due to a negative\n      # example), we pad box_ids so that the resulting number of boxes is 0.\n      num_boxes = math_ops.reduce_max(\n          array_ops.pad(box_ids + 1, [[0, 0], [0, 1]]), axis=1)\n      return num_boxes\n\n    num_boxes = _compute_num_boxes(keys_to_tensors[self._keys[0]])\n    asserts = []\n    if self._check_consistency:\n      for i in range(1, len(self._keys)):\n        cur_num_boxes = _compute_num_boxes(keys_to_tensors[self._keys[i]])\n        asserts.append(check_ops.assert_equal(num_boxes, cur_num_boxes))\n\n    with ops.control_dependencies(asserts):\n      return array_ops.identity(num_boxes)\n\n\nclass KeypointsSequence(ItemHandler):\n  \"\"\"An ItemHandler that concatenates SparseTensors to Keypoints.\n  \"\"\"\n\n  def __init__(self, keys=None, prefix=None, return_dense=True,\n               default_value=-1.0):\n    \"\"\"Initialize the keypoints handler.\n\n    Args:\n      keys: A list of two key names representing the y and x coordinates in the\n        Example or SequenceExample.\n      prefix: An optional prefix for each of the keypoint keys in the Example\n        or SequenceExample. If provided, `prefix` is prepended to each key in\n        `keys`.\n      return_dense: if True, returns a dense tensor; if False, returns as\n        sparse tensor.\n      default_value: The value used when the `tensor_key` is not found in a\n        particular `TFExample`.\n\n    Raises:\n      ValueError: if keys is not `None` and also not a list of exactly 2 keys\n    \"\"\"\n    if keys is None:\n      keys = ['y', 'x']\n    elif len(keys) != 2:\n      raise ValueError('KeypointsSequence expects 2 keys but got {}'.format(\n          len(keys)))\n    self._prefix = prefix\n    self._keys = keys\n    self._full_keys = [prefix + k for k in keys]\n    self._return_dense = return_dense\n    self._default_value = default_value\n    super(KeypointsSequence, self).__init__(self._full_keys)\n\n  def tensors_to_item(self, keys_to_tensors):\n    \"\"\"Maps the given dictionary of tensors to a concatenated list of keypoints.\n\n    Args:\n      keys_to_tensors: a mapping of TF-Example keys to parsed tensors.\n\n    Returns:\n      [time, num_keypoints, 2] tensor of keypoint coordinates, in order [y, x].\n          Whether the tensor is a SparseTensor or a dense Tensor is determined\n          by the return_dense parameter. Empty positions in the sparse tensor\n          are filled with -1.0 values.\n    \"\"\"\n    coordinates = []\n    for key in self._full_keys:\n      value = keys_to_tensors[key]\n      expanded_dims = array_ops.concat(\n          [math_ops.to_int64(array_ops.shape(value)),\n           constant_op.constant([1], dtype=dtypes.int64)], 0)\n      coordinate = sparse_ops.sparse_reshape(value, expanded_dims)\n      coordinates.append(coordinate)\n    keypoints = sparse_ops.sparse_concat(2, coordinates)\n    if self._return_dense:\n      keypoints = sparse_ops.sparse_tensor_to_dense(\n          keypoints, default_value=self._default_value)\n    return keypoints\n\n\nclass TFExampleDecoder(data_decoder.DataDecoder):\n  \"\"\"A decoder for TensorFlow Examples.\n\n  Decoding Example proto buffers is comprised of two stages: (1) Example parsing\n  and (2) tensor manipulation.\n\n  In the first stage, the tf.io.parse_example function is called with a list of\n  FixedLenFeatures and SparseLenFeatures. These instances tell TF how to parse\n  the example. The output of this stage is a set of tensors.\n\n  In the second stage, the resulting tensors are manipulated to provide the\n  requested 'item' tensors.\n\n  To perform this decoding operation, an ExampleDecoder is given a list of\n  ItemHandlers. Each ItemHandler indicates the set of features for stage 1 and\n  contains the instructions for post_processing its tensors for stage 2.\n  \"\"\"\n\n  def __init__(self, keys_to_features, items_to_handlers):\n    \"\"\"Constructs the decoder.\n\n    Args:\n      keys_to_features: a dictionary from TF-Example keys to either\n        tf.io.VarLenFeature or tf.io.FixedLenFeature instances. See tensorflow's\n        parsing_ops.py.\n      items_to_handlers: a dictionary from items (strings) to ItemHandler\n        instances. Note that the ItemHandler's are provided the keys that they\n        use to return the final item Tensors.\n    \"\"\"\n    self._keys_to_features = keys_to_features\n    self._items_to_handlers = items_to_handlers\n\n  def list_items(self):\n    \"\"\"See base class.\"\"\"\n    return list(self._items_to_handlers.keys())\n\n  def decode(self, serialized_example, items=None):\n    \"\"\"Decodes the given serialized TF-example.\n\n    Args:\n      serialized_example: a serialized TF-example tensor.\n      items: the list of items to decode. These must be a subset of the item\n        keys in self._items_to_handlers. If `items` is left as None, then all\n        of the items in self._items_to_handlers are decoded.\n\n    Returns:\n      the decoded items, a list of tensor.\n    \"\"\"\n    example = parsing_ops.parse_single_example(serialized_example,\n                                               self._keys_to_features)\n\n    # Reshape non-sparse elements just once, adding the reshape ops in\n    # deterministic order.\n    for k in sorted(self._keys_to_features):\n      v = self._keys_to_features[k]\n      if isinstance(v, parsing_ops.FixedLenFeature):\n        example[k] = array_ops.reshape(example[k], v.shape)\n\n    if not items:\n      items = self._items_to_handlers.keys()\n\n    outputs = []\n    for item in items:\n      handler = self._items_to_handlers[item]\n      keys_to_tensors = {key: example[key] for key in handler.keys}\n      outputs.append(handler.tensors_to_item(keys_to_tensors))\n    return outputs\n\n\nclass TFSequenceExampleDecoder(data_decoder.DataDecoder):\n  \"\"\"A decoder for TensorFlow SequenceExamples.\n\n  Decoding SequenceExample proto buffers is comprised of two stages:\n  (1) Example parsing and (2) tensor manipulation.\n\n  In the first stage, the tf.parse_single_sequence_example function is called\n  with a list of FixedLenFeatures and SparseLenFeatures. These instances tell TF\n  how to parse the example. The output of this stage is a set of tensors.\n\n  In the second stage, the resulting tensors are manipulated to provide the\n  requested 'item' tensors.\n\n  To perform this decoding operation, a SequenceExampleDecoder is given a list\n  of ItemHandlers. Each ItemHandler indicates the set of features for stage 1\n  and contains the instructions for post_processing its tensors for stage 2.\n  \"\"\"\n\n  def __init__(self, keys_to_context_features, keys_to_sequence_features,\n               items_to_handlers):\n    \"\"\"Constructs the decoder.\n\n    Args:\n      keys_to_context_features: a dictionary from TF-SequenceExample context\n        keys to either tf.VarLenFeature or tf.FixedLenFeature instances.\n        See tensorflow's parsing_ops.py.\n      keys_to_sequence_features: a dictionary from TF-SequenceExample sequence\n        keys to either tf.VarLenFeature or tf.FixedLenSequenceFeature instances.\n        See tensorflow's parsing_ops.py.\n      items_to_handlers: a dictionary from items (strings) to ItemHandler\n        instances. Note that the ItemHandler's are provided the keys that they\n        use to return the final item Tensors.\n\n    Raises:\n      ValueError: if the same key is present for context features and sequence\n        features.\n    \"\"\"\n    unique_keys = set()\n    unique_keys.update(keys_to_context_features)\n    unique_keys.update(keys_to_sequence_features)\n    if len(unique_keys) != (\n      len(keys_to_context_features) + len(keys_to_sequence_features)):\n      # This situation is ambiguous in the decoder's keys_to_tensors variable.\n      raise ValueError(list(keys_to_context_features.keys()), list(keys_to_sequence_features.keys()))\n\n    self._keys_to_context_features = keys_to_context_features\n    self._keys_to_sequence_features = keys_to_sequence_features\n    self._items_to_handlers = items_to_handlers\n\n  def list_items(self):\n    \"\"\"See base class.\"\"\"\n    return self._items_to_handlers.keys()\n\n  def decode(self, serialized_example, items=None):\n    \"\"\"Decodes the given serialized TF-SequenceExample.\n\n    Args:\n      serialized_example: a serialized TF-SequenceExample tensor.\n      items: the list of items to decode. These must be a subset of the item\n        keys in self._items_to_handlers. If `items` is left as None, then all\n        of the items in self._items_to_handlers are decoded.\n\n    Returns:\n      the decoded items, a list of tensor.\n    \"\"\"\n\n    context, feature_list = parsing_ops.parse_single_sequence_example(\n        serialized_example, self._keys_to_context_features,\n        self._keys_to_sequence_features)\n\n    # Reshape non-sparse elements just once:\n    for k in self._keys_to_context_features:\n      v = self._keys_to_context_features[k]\n      if isinstance(v, parsing_ops.FixedLenFeature):\n        context[k] = array_ops.reshape(context[k], v.shape)\n\n    if not items:\n      items = self._items_to_handlers.keys()\n\n    outputs = []\n    for item in items:\n      handler = self._items_to_handlers[item]\n      keys_to_tensors = {\n          key: context[key] if key in context else feature_list[key]\n          for key in handler.keys\n      }\n      outputs.append(handler.tensors_to_item(keys_to_tensors))\n    return outputs\n'''","metadata":{"execution":{"iopub.status.busy":"2024-05-29T11:48:45.801995Z","iopub.execute_input":"2024-05-29T11:48:45.802297Z","iopub.status.idle":"2024-05-29T11:48:45.838035Z","shell.execute_reply.started":"2024-05-29T11:48:45.802273Z","shell.execute_reply":"2024-05-29T11:48:45.837054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('/opt/conda/lib/python3.10/site-packages/tf_slim/data/tfexample_decoder.py','w') as f:    \n    f.write(fix_bug_to_use_gpu)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T11:48:45.839228Z","iopub.execute_input":"2024-05-29T11:48:45.839570Z","iopub.status.idle":"2024-05-29T11:48:45.850059Z","shell.execute_reply.started":"2024-05-29T11:48:45.839537Z","shell.execute_reply":"2024-05-29T11:48:45.849263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"markdown","source":"* model_dir: is a folder path to save your model while training \n* pipeline_config_path: path to your file pipeline\n* num_train_step: number of epochs you want to train (recommend > 1000)  ","metadata":{}},{"cell_type":"code","source":"!python /kaggle/working/Tensorflow/models/research/object_detection/model_main_tf2.py \\\n--pipeline_config_path=/kaggle/working/Tensorflow/pretrained/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/pipeline.config \\\n--model_dir=/kaggle/working/Tensorflow/output_model --num_train_steps=20000 ","metadata":{"execution":{"iopub.status.busy":"2024-05-23T14:42:00.092826Z","iopub.execute_input":"2024-05-23T14:42:00.093707Z","iopub.status.idle":"2024-05-23T14:48:40.389857Z","shell.execute_reply.started":"2024-05-23T14:42:00.093639Z","shell.execute_reply":"2024-05-23T14:48:40.388858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"create a folder to export a model after finishing the training  ","metadata":{}},{"cell_type":"code","source":"!mkdir /kaggle/working/export_model","metadata":{"execution":{"iopub.status.busy":"2024-05-27T10:56:37.975630Z","iopub.execute_input":"2024-05-27T10:56:37.975951Z","iopub.status.idle":"2024-05-27T10:56:38.941412Z","shell.execute_reply.started":"2024-05-27T10:56:37.975921Z","shell.execute_reply":"2024-05-27T10:56:38.940162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working/Tensorflow/models/research\n%cp /kaggle/working/Tensorflow/models/research/object_detection/exporter_main_v2.py .\n\n!python exporter_main_v2.py \\\n--trained_checkpoint_dir=/kaggle/working/Tensorflow/output_model \\\n--pipeline_config_path=/kaggle/working/Tensorflow/pretrained/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/pipeline.config \\\n--output_directory=/kaggle/working/export_model","metadata":{"execution":{"iopub.status.busy":"2024-05-23T14:34:17.559992Z","iopub.execute_input":"2024-05-23T14:34:17.560905Z","iopub.status.idle":"2024-05-23T14:34:22.370867Z","shell.execute_reply.started":"2024-05-23T14:34:17.560856Z","shell.execute_reply":"2024-05-23T14:34:22.369751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Retrain a model if you have exported the model above ","metadata":{}},{"cell_type":"code","source":"import io\nimport os\nimport scipy.misc\nimport numpy as np\nimport six\nimport time\nimport glob\nfrom IPython.display import display\n\nfrom six import BytesIO\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom PIL import Image, ImageDraw, ImageFont\n\nimport tensorflow as tf\nfrom object_detection.utils import ops as utils_ops\nfrom object_detection.utils import label_map_util\nfrom object_detection.utils import visualization_utils as vis_util\n\n#Load model\ntf.keras.backend.clear_session()\nmodel = tf.saved_model.load(\"/kaggle/input/ssd-model-epoch15k/saved_model\")","metadata":{"execution":{"iopub.status.busy":"2024-05-28T16:02:08.501113Z","iopub.execute_input":"2024-05-28T16:02:08.501623Z","iopub.status.idle":"2024-05-28T16:02:20.551011Z","shell.execute_reply.started":"2024-05-28T16:02:08.501586Z","shell.execute_reply":"2024-05-28T16:02:20.550103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir /kaggle/working/model_pretrained\n!cp -r /kaggle/input/eval-model/export_model_1500epoch /kaggle/working/model_pretrained","metadata":{"execution":{"iopub.status.busy":"2024-05-29T11:49:40.342794Z","iopub.execute_input":"2024-05-29T11:49:40.343503Z","iopub.status.idle":"2024-05-29T11:49:45.653326Z","shell.execute_reply.started":"2024-05-29T11:49:40.343472Z","shell.execute_reply":"2024-05-29T11:49:45.652211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = '''\nmodel {\n  ssd {\n    num_classes: 19\n    image_resizer {\n      fixed_shape_resizer {\n        height: 640\n        width: 640\n      }\n    }\n    feature_extractor {\n      type: \"ssd_resnet50_v1_fpn_keras\"\n      depth_multiplier: 1.0\n      min_depth: 16\n      conv_hyperparams {\n        regularizer {\n          l2_regularizer {\n            weight: 0.0004\n          }\n        }\n        initializer {\n          truncated_normal_initializer {\n            mean: 0.0\n            stddev: 0.03\n          }\n        }\n        activation: RELU_6\n        batch_norm {\n          decay: 0.997\n          scale: true\n          epsilon: 0.001\n        }\n      }\n      override_base_feature_extractor_hyperparams: true\n      fpn {\n        min_level: 3\n        max_level: 7\n      }\n    }\n    box_coder {\n      faster_rcnn_box_coder {\n        y_scale: 10.0\n        x_scale: 10.0\n        height_scale: 5.0\n        width_scale: 5.0\n      }\n    }\n    matcher {\n      argmax_matcher {\n        matched_threshold: 0.5\n        unmatched_threshold: 0.5\n        ignore_thresholds: false\n        negatives_lower_than_unmatched: true\n        force_match_for_each_row: true\n        use_matmul_gather: true\n      }\n    }\n    similarity_calculator {\n      iou_similarity {\n      }\n    }\n    box_predictor {\n      weight_shared_convolutional_box_predictor {\n        conv_hyperparams {\n          regularizer {\n            l2_regularizer {\n              weight: 0.0004\n            }\n          }\n          initializer {\n            random_normal_initializer {\n              mean: 0.0\n              stddev: 0.01\n            }\n          }\n          activation: RELU_6\n          batch_norm {\n            decay: 0.997\n            scale: true\n            epsilon: 0.001\n          }\n        }\n        depth: 256\n        num_layers_before_predictor: 4\n        kernel_size: 3\n        class_prediction_bias_init: -4.6\n      }\n    }\n    anchor_generator {\n      multiscale_anchor_generator {\n        min_level: 3\n        max_level: 7\n        anchor_scale: 4.0\n        aspect_ratios: 1.0\n        aspect_ratios: 2.0\n        aspect_ratios: 0.5\n        scales_per_octave: 2\n      }\n    }\n    post_processing {\n      batch_non_max_suppression {\n        score_threshold: 1e-08\n        iou_threshold: 0.6\n        max_detections_per_class: 100\n        max_total_detections: 100\n        use_static_shapes: false\n      }\n      score_converter: SIGMOID\n    }\n    normalize_loss_by_num_matches: true\n    loss {\n      localization_loss {\n        weighted_smooth_l1 {\n        }\n      }\n      classification_loss {\n        weighted_sigmoid_focal {\n          gamma: 2.0\n          alpha: 0.25\n        }\n      }\n      classification_weight: 1.0\n      localization_weight: 1.0\n    }\n    encode_background_as_zeros: true\n    normalize_loc_loss_by_codesize: true\n    inplace_batchnorm_update: true\n    freeze_batchnorm: false\n  }\n}\ntrain_config {\n  batch_size: 22\n  data_augmentation_options {\n    random_horizontal_flip {\n    }\n  }\n  data_augmentation_options {\n    random_crop_image {\n      min_object_covered: 0.0\n      min_aspect_ratio: 0.75\n      max_aspect_ratio: 3.0\n      min_area: 0.75\n      max_area: 1.0\n      overlap_thresh: 0.0\n    }\n  }\n  sync_replicas: true\n  optimizer {\n    momentum_optimizer {\n      learning_rate {\n        cosine_decay_learning_rate {\n          learning_rate_base: 0.04\n          total_steps: 25000\n          warmup_learning_rate: 0.013333\n          warmup_steps: 2000\n        }\n      }\n      momentum_optimizer_value: 0.9\n    }\n    use_moving_average: false\n  }\n  fine_tune_checkpoint: \"/kaggle/working/model_pretrained/export_model_1500epoch/checkpoint/ckpt-0\"\n  num_steps: 25000\n  startup_delay_steps: 0.0\n  replicas_to_aggregate: 8\n  max_number_of_boxes: 100\n  unpad_groundtruth_tensors: false\n  fine_tune_checkpoint_type: \"full\"\n  use_bfloat16: true\n  fine_tune_checkpoint_version: V2\n}\ntrain_input_reader {\n  label_map_path: \"/kaggle/working/Tensorflow/PokemonDetection_SSD/label_map.txt\"\n  tf_record_input_reader {\n    input_path: \"/kaggle/working/Tensorflow/PokemonDetection_SSD/tfrecord_data/train.record\"\n  }\n}\neval_config {\n  metrics_set: \"coco_detection_metrics\"\n  use_moving_averages: false\n}\neval_input_reader {\n  label_map_path: \"/kaggle/working/Tensorflow/PokemonDetection_SSD/label_map.txt\"\n  shuffle: false\n  num_epochs: 1\n  tf_record_input_reader {\n    input_path: \"/kaggle/working/Tensorflow/PokemonDetection_SSD/tfrecord_data/test.record\"\n  }\n}\n\n'''","metadata":{"execution":{"iopub.status.busy":"2024-05-29T11:49:45.656192Z","iopub.execute_input":"2024-05-29T11:49:45.656765Z","iopub.status.idle":"2024-05-29T11:49:45.665781Z","shell.execute_reply.started":"2024-05-29T11:49:45.656724Z","shell.execute_reply":"2024-05-29T11:49:45.664916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# num_workers = 2 (to use multi gpus)","metadata":{}},{"cell_type":"code","source":"with open('/kaggle/working/model_pretrained/export_model_1500epoch/pipeline.config', 'w') as fp:\n    fp.write(config)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T11:49:45.666944Z","iopub.execute_input":"2024-05-29T11:49:45.667251Z","iopub.status.idle":"2024-05-29T11:49:45.679716Z","shell.execute_reply.started":"2024-05-29T11:49:45.667228Z","shell.execute_reply":"2024-05-29T11:49:45.678849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python /kaggle/working/Tensorflow/models/research/object_detection/model_main_tf2.py --num_workers=2 --model_dir=/kaggle/working/model_pretrained/export_model_1500epoch/saved_model --pipeline_config_path=/kaggle/working/model_pretrained/export_model_1500epoch/pipeline.config --num_train_steps=30000 ","metadata":{"execution":{"iopub.status.busy":"2024-05-29T11:49:46.366048Z","iopub.execute_input":"2024-05-29T11:49:46.366447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !python /kaggle/working/Tensorflow/models/research/object_detection/model_main_tf2.py --model_dir=/kaggle/working/model_pretrained/export_model_1500epoch/saved_model \\\n#     --pipeline_config_path=/kaggle/working/model_pretrained/export_model_1500epoch/pipeline.config \\\n#     --model_dir=/kaggle/working/model_pretrained/export_model_1500epoch/saved_model \\\n#     --checkpoint_dir= /kaggle/working/model_pretrained/export_model_1500epoch/saved_model/checkpoint\\\n#     --alsologtostderr","metadata":{"execution":{"iopub.status.busy":"2024-05-28T08:57:44.678850Z","iopub.execute_input":"2024-05-28T08:57:44.679302Z","iopub.status.idle":"2024-05-28T09:04:24.054165Z","shell.execute_reply.started":"2024-05-28T08:57:44.679268Z","shell.execute_reply":"2024-05-28T09:04:24.052839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Eval","metadata":{}},{"cell_type":"markdown","source":"model_dir: path to your model  ","metadata":{}},{"cell_type":"code","source":"!python /kaggle/working/Tensorflow/models/research/object_detection/model_main_tf2.py \\\n    --pipeline_config_path=/kaggle/working/model_pretrained/export_model_1500epoch/pipeline.config \\\n    --model_dir=/kaggle/working/model_pretrained/export_model_1500epoch/saved_model \\\n    --checkpoint_dir=/kaggle/working/model_pretrained/export_model_1500epoch/saved_model \\\n    --eval_timeout=0 \\\n    --alsologtostderr","metadata":{"execution":{"iopub.status.busy":"2024-05-23T16:15:15.297298Z","iopub.execute_input":"2024-05-23T16:15:15.297664Z","iopub.status.idle":"2024-05-23T16:20:34.390334Z","shell.execute_reply.started":"2024-05-23T16:15:15.297635Z","shell.execute_reply":"2024-05-23T16:20:34.389348Z"},"trusted":true},"execution_count":null,"outputs":[]}]}